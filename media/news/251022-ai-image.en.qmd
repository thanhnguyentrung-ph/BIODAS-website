---
title: "Artificial Intelligence in Medical Imaging: Present and Future"
subtitle: "The Current and Future State of AI Interpretation of Medical Images"
title-block-banner: "#313d5d"
title-block-banner-color: "#ffffff"
description: ""
categories:
  - AI
  - Medical Images
  - Diagnosis
  - CT
  - MRI
author: "Pranav Rajpurkar et al. | Translated and edited by BIODAS Team"
date: 10/22/2025 # mm/dd/yyyy
toc: true
image: https://www.onixnet.com/wp-content/uploads/2023/03/How-AI-Powered-Medical-Imaging-is-Transforming-Healthcare.jpg
aliases: 
  - /media/news/
---

![](https://www.alcimed.com/wp-content/uploads/2023/09/ai-medical-imaging-1.jpg){fig-align="center" class="img-cover"}



```{=html}
<!-- CSS file attachment -->
<link rel="stylesheet" href="section-news-template.css">

<!-- Font Awesome CDN (for social media icons) -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.0/css/all.min.css">


<body class="section-news-template">
<div class="layout">

  <!-- Sidebar TOC -->
  <nav class="toc">
    <h3>Table of Contents</h3>
    <ol>
      <li><a href="#sec-1">The Problem</a></li>
      <li><a href="#sec-2">The Role of AI in Medical Imaging</a></li>
      <li><a href="#sec-3">Clinical and Systemic Benefits</a></li>
      <li><a href="#sec-4">Key Implementation Challenges</a></li>
      <li><a href="#sec-5">Future Directions: Foundation & Multimodal AI</a></li>
      <li><a href="#sec-6">Application Recommendations for Vietnam</a></li>
      <li><a href="#sec-7">Conclusion</a></li>
      <li><a href="#sec-8">References</a></li>
    </ol>
  </nav>

  <!-- Main content -->
  <main class="content">

    <!-- =============================
     INTRO SECTION – BIODAS ARTICLE INTRO
     ============================= -->
    <section>
      <p>
        The academic review article 
        <strong>
          <a href="https://doi.org/10.1056/nejmra2301725"
            target="_blank"
            rel="noopener noreferrer"
            style="color:#313d5d; text-decoration:underline;">
            “AI in Medicine – The Current and Future State of AI Interpretation of Medical Images”
          </a>
        </strong>
        by <em>Pranav Rajpurkar, Ph.D.</em> and <em>Matthew P. Lungren, M.D., M.P.H.</em>, edited by 
        <em>Jeffrey M. Drazen, M.D.</em> and two guest editors <em>Isaac S. Kohane, M.D., Ph.D.</em> and <em>Tze-Yun Leong, Ph.D.</em>, 
        was published in <em>The New England Journal of Medicine</em> – one of the world's leading medical journals.
      </p>
      <p>
        The research focuses on the current and future state of <strong>artificial intelligence (AI) in the interpretation of medical images</strong>, 
        a rapidly developing field with profound impacts on diagnosis, treatment, and clinical decision-making. 
        The <strong>BIODAS Team</strong> is pleased to present and share the content of this article to help the medical community better understand the 
        role, potential, and challenges of AI in medical image analysis, thereby opening up a comprehensive perspective on how technology is reshaping medical practice in the digital age.
      </p>
    </section>

    <!-- 1. The Problem -->
    <section id="sec-1">
      <h2>1. The Problem</h2>
      <p>
        The interpretation of medical images is central to diagnostic practice, traditionally relying on the pattern recognition abilities and accumulated experience of physicians. 
        In the last decade, AI (especially deep learning) has made a leap in performance in many specialties: dermatology, cardiology (ECG), ophthalmology, 
        histopathology, chest X-ray, CT, MRI, and ultrasound<sup>1–5</sup>. 
        Although there are <em>hundreds</em> of clinical applications and many approved devices, the transition from “model evidence” to “clinical value” 
        still needs to overcome barriers of <em>generalizability</em>, <em>interpretability</em>, <em>field evidence</em>, and <em>risk management</em><sup>6–14</sup>.
      </p>
      <p>
        The NEJM article summarizes the progress, applications, clinical evidence, implementation challenges, and future prospects of AI in image interpretation, 
        emphasizing two main axes: (i) <strong>clinical and procedural efficiency</strong> (from lesion detection to emergency case prioritization – triage), 
        and (ii) <strong>technological foundation</strong> (from specialized models to multi-task, multi-modal foundation models).
      </p>
    </section>

    <figure>
      <div class="img-wrap">
        <img src="https://www.nejm.org/cms/asset/8982ebc1-9118-4581-b673-455d5416ea30/nejmra2301725_f1.jpg" class="img" />
      </div>
      <figcaption>Image: Illustration of current applications of artificial intelligence (AI) in the field of medical imaging, focusing on three main clinical functions: triage, detection, and diagnosis. Typical AI modules include CADt (computer-aided detection for triage), CADe (computer-aided detection for characterizing abnormalities), and CADx (computer-aided detection for diagnosis). In addition, AI is also used in image reconstruction and noise reduction processes to improve the quality of the output image. AI applications not directly related to medical imaging (non-imaging) are not presented in the figure. CT stands for computed tomography (Source: Rajpurkar P et al., 2023).
      </figcaption>
    </figure>

    <!-- 2. The Role of AI -->
    <section id="sec-2">
      <h2>2. The Role of AI in Medical Imaging</h2>
      <p>
        AI provides support at multiple levels: <em>preprocessing</em> (noise reduction, CT/MR reconstruction, artifact correction), <em>detection/segmentation</em> (lesion detection/segmentation), 
        <em>classification</em> (benign/malignant, inflammatory/infectious…), <em>quantification</em> (volume, density, flow), and <em>prediction</em> (prognosis of events, treatment response)<sup>6–14</sup>. 
        At the system level, AI contributes to <strong>prioritizing urgent cases</strong> (e.g., intracranial hemorrhage, large vessel occlusion, pneumothorax, pulmonary embolism), 
        classifying severity levels, and coordinating workflow<sup>11–14,20</sup>.
      </p>
      <div class="card">
        <ul>
          <li><strong>Quantification & imaging biomarkers:</strong> measuring bone density, visceral fat, liver fat; quantifying brain structures, left ventricular function, coronary flow; supporting chronic risk assessment<sup>9–10</sup>.</li>
          <li><strong>Processing prioritization:</strong> triage models detect acute lesions to move them to the top of the reading list, shortening the time to intervention<sup>12,20</sup>.</li>
          <li><strong>Image quality improvement:</strong> deep learning for slice reconstruction and reduction of radiation dose/scan time while maintaining diagnostic quality<sup>9</sup>.</li>
          <li><strong>Clinical prediction:</strong> from image data, AI predicts progression (e.g., TBI, cancer) or guides novices in endoscopy/ultrasound to achieve acceptable diagnostic quality<sup>13–14</sup>.</li>
        </ul>
      </div>
      <p>
        Many studies have demonstrated the effectiveness of human-machine collaboration: AI improves sensitivity/specificity, reduces reading time, and standardizes assessment; 
        at the same time, “AI-assisted – physician-decides” models often achieve higher levels of trust and effectiveness than AI operating independently<sup>16–20</sup>.
      </p>
    </section>

    <!-- 3. Benefits -->
    <section id="sec-3">
      <h2>3. Clinical and Systemic Benefits</h2>
      <p>
        Clinically, AI can <em>accelerate</em> and <em>improve the accuracy</em> of diagnosis, detect small lesions, 
        reduce variability among physicians, and support the creation of structured reports. Systemically, AI helps <em>prioritize urgent cases</em>, 
        <em>optimize workflow</em>, allocate resources, and shorten the time to treatment<sup>11–14,20</sup>. 
        In some tasks, AI also <em>predicts outcomes</em> or <em>suggests the need for additional tests/imaging</em>, 
        moving towards personalized care pathways<sup>1–5,10</sup>.
      </p>

      <div class="callout">
        <ul>
          <li><strong>Reading efficiency & quality:</strong> assists physicians in detecting abnormalities (small pulmonary nodules, microcalcifications, microhemorrhages) and reduces omissions<sup>16–20</sup>.</li>
          <li><strong>Standardization of practice:</strong> reduces discrepancies between experts/units; increases the reproducibility of results<sup>16,19</sup>.</li>
          <li><strong>Support for resource-limited areas:</strong> handheld ultrasound/low-field MRI combined with AI expands diagnostic access at the primary care level<sup>10,14</sup>.</li>
        </ul>
      </div>
    </section>

    <figure>
      <div class="img-wrap">
        <img src="https://www.nejm.org/cms/10.1056/NEJMra2301725/asset/6bd4b8cf-aed2-4136-8413-3e4dbeee3f4c/assets/images/large/nejmra2301725_f2.jpg" class="img" />
      </div>
      <figcaption>Image: Validating the generalizability of AI systems in medical imaging. The three core components in validating the generalizability of AI systems in the field of medical imaging include (1) collaboration between physicians and AI, (2) transparency, and (3) post-deployment monitoring. First, physician-AI collaboration emphasizes the need to shift the focus of evaluation from solely considering the performance of the AI model operating independently to assessing the practical value of AI as a supportive tool in the actual clinical workflow. Second, transparency refers to the requirement to enhance the rigor of AI model information disclosure, through the application of checklists and the public release of medical imaging datasets for validation and research. Finally, post-deployment monitoring includes establishing mechanisms for receiving feedback from clinicians and applying a continual learning strategy to regularly update and calibrate the model, ensuring that AI maintains its effectiveness and safety in a real-world environment (Source: Rajpurkar P et al., 2023).</figcaption>
    </figure>

    <!-- 4. Challenges -->
    <section id="sec-4">
      <h2>4. Key Implementation Challenges</h2>
      <div class="card">
        <p>
          <strong>(i) Generalizability & dataset/domain shift.</strong> 
          A model's performance may decline when applied to different hospitals, scanners, populations, or protocols. 
          It is necessary to design multi-center trials, prospective/cluster-randomized trials, and domain adaptation/data standardization strategies<sup>6–8,15</sup>.
        </p>
        <p>
          <strong>(ii) Transparency & explainability (black box).</strong> 
          The lack of transparency reduces trust and makes it difficult to integrate into clinical decision-making. Solutions include: 
          comprehensive reporting (dataset statements/model cards), local/global explanation methods, and UI design that emphasizes uncertainty<sup>16,21–22</sup>.
        </p>
        <p>
          <strong>(iii) Lifecycle management & post-deployment drift.</strong> 
          Model performance changes over time due to changes in population/equipment/protocols. It is necessary to monitor performance, alert for drift, 
          update/retrain in a controlled manner, and conduct periodic audits<sup>23</sup>.
        </p>
        <p>
          <strong>(iv) Data, fairness & privacy.</strong> 
          Data representativeness is a prerequisite to avoid bias. 
          Multi-center alliances, federated learning, and standardized metadata sharing help expand data while protecting privacy<sup>21–22</sup>.
        </p>
        <p>
          <strong>(v) Regulation & evidence.</strong> 
          A legal framework is needed that reflects the specifics of AI/ML medical software, supports approval through a total product lifecycle, and allows for adaptive updates (“learning” devices), 
          while also requiring <em>prospective clinical evidence</em> of safety and effectiveness, not just AUROC in training<sup>6–8,23</sup>.
        </p>
      </div>
    </section>

    <!-- 5. Future -->
    <section id="sec-5">
      <h2>5. Future Directions: Foundation &amp; Multimodal AI</h2>
      <p>
        <strong>Foundation models</strong> and <strong>multimodal AI</strong> are shifting the focus from “narrow-single-task” models 
        to <em>generalist medical AI</em> capable of simultaneously processing images, natural language (reports, EHR notes), and lab/vital data. 
        Self-supervised learning on large unlabeled data repositories helps extract rich representations, enhancing the effectiveness of “transfer learning” for specific clinical tasks<sup>24–25</sup>.
      </p>
      <p>
        Combining <strong>large language models (LLMs)</strong> with medical vision allows for: 
        generating X-ray/CT/MRI reports in a standard style, explaining findings to patients, extracting tasks from clinical orders, 
        and supporting evidence-based <em>reasoning</em>. Recent studies show that LLMs encode significant clinical knowledge and can support decision-making, 
        provided they are rigorously validated and designed with a “human-in-the-loop”<sup>26–27</sup>.
      </p>
      <p>
        The practical vision is an <em>“AI collaborator”</em> architecture where the model assists physicians in detection, report drafting, case prioritization, 
        risk prediction, and suggesting care pathways. Success depends on <strong>co-design</strong> with physicians/process experts, 
        <strong>transparency</strong> of data and models, and <strong>post-deployment monitoring</strong> according to quality standards.
      </p>
    </section>

    <figure>
      <div class="img-wrap">
        <img src="https://www.nejm.org/cms/10.1056/NEJMra2301725/asset/3c9bcea8-a343-47f6-92b6-4c4c0b7d25d9/assets/images/large/nejmra2301725_f3.jpg" class="img" />
      </div>
      <figcaption>Image: The potential of generalist AI models in medical imaging. Generalist medical AI models offer enormous potential to comprehensively transform the field of medical imaging. These models are capable of generating complete diagnostic imaging reports, including clinical descriptions and interpretations synthesized from various data sources such as medical images, clinical context, and previous images. In addition, these models can link specific regions in an image to corresponding language descriptions, customize responses according to the user, and generate diagnostic results in a fully interpretive context, helping physicians better understand the clinical significance of the findings. Notably, these generalist models are also expected to be able to adapt flexibly to new practice environments and technologies, thereby maintaining effectiveness as imaging techniques or data change. In addition to image data, these models can integrate “omics” data (including genomics, epigenomics, proteomics, and metabolomics), opening up new avenues for personalized diagnosis and treatment based on multi-layered biological data. (Source: Rajpurkar P et al., 2023).</figcaption>
    </figure>

    <!-- 6. Recommendations for Vietnam -->
    <section id="sec-6">
      <h2>6. Application Recommendations for Vietnam</h2>
      <div class="callout">
        <ul>
          <li><strong>Priority applications with clear benefits:</strong> chest X-ray triage modules (pneumothorax, hemorrhage, PE), stroke detection on CT, support for primary care/mobile ultrasound; pilot implementation at central hospitals before expansion.</li>
          <li><strong>“AI made in Vietnam” model:</strong> train/fine-tune on Vietnamese data to increase generalizability; encourage multi-center alliances and <em>federated learning</em> to expand data while ensuring privacy<sup>22</sup>.</li>
          <li><strong>Data & legal standards:</strong> issue standard profiles (DICOM + standardized metadata), require transparent reporting (dataset/model cards), monitor drift, and have a lifecycle-based approval-update system<sup>23</sup>.</li>
          <li><strong>Training & operation:</strong> “AI for Radiologists & Sonographers” programs on reading results, understanding uncertainty, and human-machine decision-making; design UI/UX that displays case-level <em>rationale</em>.</li>
          <li><strong>Value measurement:</strong> clinical and operational outcome criteria (time to treatment, omission rate, reading capacity, cost), prospective/cluster-randomized trials instead of just comparing AUC.</li>
        </ul>
      </div>
    </section>

    <!-- 7. Conclusion -->
    <section id="sec-7">
      <h2>7. Conclusion</h2>
      <p>
        AI in medical imaging is moving from single-task models to generalist multimodal platforms, 
        with growing evidence of clinical and systemic benefits. However, <em>generalizability</em>, 
        <em>transparency</em>, and <em>post-deployment monitoring</em> are prerequisites for achieving sustainable value. 
        Instead of replacing, AI acts as a “collaborator” that helps physicians reduce errors, standardize quality, and expand diagnostic access – 
        especially useful for resource-limited settings. With a suitable data-legal-training strategy, 
        Vietnam can “leapfrog” to leverage the AI wave for digital health transformation.
      </p>
    </section>

    <!-- 8. References -->
    <section id="sec-8">
      <h2>8. References</h2>

      <div class="ref">
        <p><strong>NEJM Review</strong></p>
        <p>
          Rajpurkar P, Lungren MP, Drazen JM, Kohane IS, Leong TY. 
          <em>AI in Medicine — The Current and Future State of AI Interpretation of Medical Images.</em> 
          <strong>N Engl J Med.</strong> 2023;388:1981–1990. 
          <a href="https://doi.org/10.1056/NEJMra2301725" target="_blank" style="color:#0284c7;text-decoration:underline;">
            doi:10.1056/NEJMra2301725
          </a>.
        </p>
      </div>

      <div class="ref">
        <p><strong>Related Literature</strong></p>
        <ol>
          <li>
            Topol EJ, et al. Artificial intelligence in health and medicine. 
            <em>Nat Med.</em> 2022;28(1):31–38. 
            <a href="https://doi.org/10.1038/s41591-021-01614-0" target="_blank">doi:10.1038/s41591-021-01614-0</a>.
          </li>
          <li>
            Jones OT, Ranmuthu CKI, Hall PN, Funston G, Walter FM. Early detection of skin cancer in the community and primary care: 
            an integrated review. <em>Lancet Digit Health.</em> 2022;4(7):e466–e476. 
            <a href="https://doi.org/10.1016/S2589-7500(22)00085-1" target="_blank">doi:10.1016/S2589-7500(22)00085-1</a>.
          </li>
          <li>
            Siontis KC, Noseworthy PA, Attia ZI, Friedman PA. Artificial intelligence–enhanced electrocardiography in cardiovascular disease. 
            <em>Nat Rev Cardiol.</em> 2021;18(7):465–478. 
            <a href="https://doi.org/10.1038/s41569-020-00503-2" target="_blank">doi:10.1038/s41569-020-00503-2</a>.
          </li>
          <li>
            Lu MY, Chen TY, Williamson DFK, et al. Artificial intelligence–based pathology predicts origins for cancers of unknown primary. 
            <em>Nature.</em> 2021;594(7861):106–110. 
            <a href="https://doi.org/10.1038/s41586-021-03512-4" target="_blank">doi:10.1038/s41586-021-03512-4</a>.
          </li>
          <li>
            Abràmoff MD, et al. Foundational considerations for artificial intelligence using ophthalmic images. 
            <em>Ophthalmology.</em> 2022;129(2S):e14–e25. 
            <a href="https://doi.org/10.1016/j.ophtha.2021.09.015" target="_blank">doi:10.1016/j.ophtha.2021.09.015</a>.
          </li>
          <li>
            Yuba M, Iwasaki K. Analysis of AI- and ML-based medical devices approved in the United States and Japan. 
            <em>Sci Rep.</em> 2022;12(1):16874. 
            <a href="https://doi.org/10.1038/s41598-022-21067-6" target="_blank">doi:10.1038/s41598-022-21067-6</a>.
          </li>
          <li>
            Tariq A, Purkayastha S, Chintha AR, et al. Clinical artificial intelligence in radiology: 
            current applications and best available evidence. 
            <em>J Am Coll Radiol.</em> 2020;17(11):1371–1381. 
            <a href="https://doi.org/10.1016/j.jacr.2020.06.004" target="_blank">doi:10.1016/j.jacr.2020.06.004</a>.
          </li>
          <li>
            Richardson ML, et al. Non-interpretive uses of artificial intelligence in radiology. 
            <em>Acad Radiol.</em> 2021;28(8):1225–1235. 
            <a href="https://doi.org/10.1016/j.acra.2020.12.010" target="_blank">doi:10.1016/j.acra.2020.12.010</a>.
          </li>
          <li>
            Wang G, Ye JC, De Man B. Deep learning for tomographic image reconstruction. 
            <em>Nat Mach Intell.</em> 2020;2(12):737–748. 
            <a href="https://doi.org/10.1038/s42256-020-00273-z" target="_blank">doi:10.1038/s42256-020-00273-z</a>.
          </li>
          <li>
            Mazurek MH, Cahn BA, Zuckerman JE, et al. Portable, low-field magnetic resonance imaging enables bedside assessment 
            of intracerebral hemorrhage. <em>Nat Commun.</em> 2021;12(1):5119. 
            <a href="https://doi.org/10.1038/s41467-021-25349-w" target="_blank">doi:10.1038/s41467-021-25349-w</a>.
          </li>
          <li>
            Brink JA, Hricak H. Radiology 2040: a vision for the future. 
            <em>Radiology.</em> 2023;306(1):69–72. 
            <a href="https://doi.org/10.1148/radiol.230089" target="_blank">doi:10.1148/radiol.230089</a>.
          </li>
          <li>
            Lee HW, Goo JM, Kim Y, et al. Effect of an artificial intelligence–based chest radiograph interpretation system 
            on radiologists’ diagnostic performance: a multicenter randomized clinical trial. 
            <em>Ann Am Thorac Soc.</em> 2022;19(10):1769–1779. 
            <a href="https://doi.org/10.1513/AnnalsATS.202201-070OC" target="_blank">doi:10.1513/AnnalsATS.202201-070OC</a>.
          </li>
          <li>
            Narang A, Bae R, Hong H, et al. Deep learning–guided acquisition of echocardiograms enables 
            cardiac screening by novices. <em>JAMA Cardiol.</em> 2021;6(5):624–632. 
            <a href="https://doi.org/10.1001/jamacardio.2020.7422" target="_blank">doi:10.1001/jamacardio.2020.7422</a>.
          </li>
          <li>
            Pokaprakarn T, et al. Estimation of gestational age from blind ultrasound sweeps using artificial intelligence. 
            <em>NEJM Evid.</em> 2022;1(5):EVIDoa2200021. 
            <a href="https://doi.org/10.1056/EVIDoa2200021" target="_blank">doi:10.1056/EVIDoa2200021</a>.
          </li>
          <li>
            Feng Y, et al. Domain adaptation for pneumonia classification from chest X-ray images. 
            <em>IEEE J Biomed Health Inform.</em> 2022;26(3):1080–1090. 
            <a href="https://doi.org/10.1109/JBHI.2021.3114060" target="_blank">doi:10.1109/JBHI.2021.3114060</a>.
          </li>
          <li>
            Langlotz CP. Will artificial intelligence replace radiologists? 
            <em>Radiol Artif Intell.</em> 2019;1(3):e190058. 
            <a href="https://doi.org/10.1148/ryai.2019190058" target="_blank">doi:10.1148/ryai.2019190058</a>.
          </li>
          <li>
            Park A, Chute C, Rajpurkar P, et al. Deep learning–assisted diagnosis of cerebral aneurysms using HeadXNet. 
            <em>JAMA Netw Open.</em> 2019;2(6):e195600. 
            <a href="https://doi.org/10.1001/jamanetworkopen.2019.5600" target="_blank">doi:10.1001/jamanetworkopen.2019.5600</a>.
          </li>
          <li>
            Tschandl P, Rinner C, Apalla Z, et al. Human–computer collaboration for skin cancer recognition. 
            <em>Nat Med.</em> 2020;26(8):1229–1234. 
            <a href="https://doi.org/10.1038/s41591-020-0942-0" target="_blank">doi:10.1038/s41591-020-0942-0</a>.
          </li>
          <li>
            Bien N, Rajpurkar P, Ball RL, et al. Deep-learning–assisted diagnosis for knee magnetic resonance imaging: 
            development and retrospective validation of MRNet. <em>PLoS Med.</em> 2018;15(11):e1002699. 
            <a href="https://doi.org/10.1371/journal.pmed.1002699" target="_blank">doi:10.1371/journal.pmed.1002699</a>.
          </li>
          <li>
            Ahn JS, Park SJ, Park B, et al. Effect of artificial intelligence–aided chest radiograph interpretation 
            on efficiency and accuracy of radiologists: a randomized clinical trial. 
            <em>JAMA Netw Open.</em> 2022;5(10):e2229289. 
            <a href="https://doi.org/10.1001/jamanetworkopen.2022.29289" target="_blank">doi:10.1001/jamanetworkopen.2022.29289</a>.
          </li>
          <li>
            Seastedt KP, Cheung L, Hu JC, et al. Global healthcare fairness: we need to share more data. 
            <em>PLOS Digit Health.</em> 2022;1(2):e0000102. 
            <a href="https://doi.org/10.1371/journal.pdig.0000102" target="_blank">doi:10.1371/journal.pdig.0000102</a>.
          </li>
          <li>
            Sheller MJ, Edwards B, Reina GA, et al. Federated learning in medicine: facilitating multi-institutional collaborations 
            without sharing patient data. <em>Sci Rep.</em> 2020;10(1):12598. 
            <a href="https://doi.org/10.1038/s41598-020-69250-1" target="_blank">doi:10.1038/s41598-020-69250-1</a>.
          </li>
          <li>
            Harvey HB, Gowda V. How the FDA regulates artificial intelligence–based medical devices. 
            <em>Acad Radiol.</em> 2020;27(1):58–61. 
            <a href="https://doi.org/10.1016/j.acra.2019.09.024" target="_blank">doi:10.1016/j.acra.2019.09.024</a>.
          </li>
          <li>
            Krishnan R, Rajpurkar P, Topol EJ. Self-supervised learning in medicine and healthcare. 
            <em>Nat Biomed Eng.</em> 2022;6(12):1346–1352. 
            <a href="https://doi.org/10.1038/s41551-022-00914-1" target="_blank">doi:10.1038/s41551-022-00914-1</a>.
          </li>
          <li>
            Fei N, Chen Z, Zhang H, et al. Towards artificial general intelligence via multimodal foundation models. 
            <em>Nat Commun.</em> 2022;13(1):3094. 
            <a href="https://doi.org/10.1038/s41467-022-30761-2" target="_blank">doi:10.1038/s41467-022-30761-2</a>.
          </li>
          <li>
            Singhal K, Azizi S, Tu T, et al. Large language models encode clinical knowledge. 
            <em>arXiv preprint</em> arXiv:2212.13138. 2022. 
            <a href="https://arxiv.org/abs/2212.13138" target="_blank">Link</a>.
          </li>
          <li>
            Lee P, Bubeck S, Petro J. Benefits, limits, and risks of GPT-4 as an artificial intelligence chatbot in medicine. 
            <em>N Engl J Med.</em> 2023;388(13):1233–1239. 
            <a href="https://doi.org/10.1056/NEJMp2301605" target="_blank">doi:10.1056/NEJMp2301605</a>.
          </li>
        </ol>
      </div>
    </section>


    <!-- SOCIAL SHARE -->
    <div class="social-section">
      <p class="share-note">Share this article</p>
      <div class="social-share">
        <a href="https://www.facebook.com/sharer/sharer.php?u=https://www.biodas.net/media/news/251022-ai-image.en.html/" class="fb" title="Share on Facebook"><i class="fab fa-facebook-f"></i></a>
        <a href="https://twitter.com/intent/tweet?status=https://www.biodas.net/media/news/251022-ai-image.en.html/" class="x" title="Share on X / Twitter"><i class="fab fa-x-twitter"></i></a>
        <a href="https://www.linkedin.com/shareArticle?mini=true&url=https://biodas.net/media/news/251022-ai-image.en.html/" class="in" title="Share on LinkedIn"><i class="fab fa-linkedin-in"></i></a>
      </div>
    </div>

  </main>
</div>

<!-- =============================
     SECTION: BIODAS NEWS & INSIGHTS
     ============================= -->
<section class="biodas-news-section">
  <div class="news-list container">
    <h2 class="section-title">OTHER NEWS</h2>
    <div id="biodas-news-grid" class="news-grid"></div>
  </div>
</section>

</body>




<!-- JS SCRIPT -->
<script src="section-news-template.js"></script>
```
